system_name: "Sheily AI Enterprise"
version: "3.1.0"
environment: "development"

# Model Configuration
model_path: "models/gemma-2-2b-it-Q4_K_M.gguf"
llama_binary_path: "tools/llama_cpp/llama-cli.exe"
model_max_tokens: 512
model_temperature: 0.7
model_threads: 4
model_timeout: 30

# Corpus Configuration
corpus_root: "data"
branches_config_path: "config/branches.json"
context_max_length: 2000
max_context_docs: 3

# Security Configuration
security_enabled: true
max_queries_per_minute: 60
max_query_length: 10000
require_authentication: false

# Logging Configuration
log_level: "INFO"
log_file: "logs/sheily_ai.log"
log_max_size: 10485760
log_backup_count: 5

# Monitoring Configuration
monitoring_enabled: true
metrics_collection_interval: 30
metrics_retention_hours: 24

# Network Configuration
host: "0.0.0.0"
port: 8000
api_prefix: "/api/v1"
cors_origins:
  - "http://localhost:3000"
  - "http://localhost:8002"
  - "http://127.0.0.1:3000"
  - "http://127.0.0.1:8002"

# Database Configuration
database_url: "sqlite:///./data/db/sheily_ai.db"
connection_pool_size: 10
connection_timeout: 30

# Cache Configuration
cache_enabled: true
cache_type: "file"
cache_url: "redis://localhost:6379"
cache_ttl: 3600

# Resource Configuration
max_memory_usage: 2048
max_cpu_usage: 80
auto_scaling_enabled: false

# Features Configuration
features:
  chat: true
  rag: true
  memory: true
  monitoring: true
  security: true
  enterprise_mode: true
