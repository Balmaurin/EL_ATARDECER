#!/usr/bin/env python3
"""
MASTER MCP ORCHESTRATOR - Enterprise System Brain
==================================================

Central intelligence coordinating all Sheily MCP Enterprise components:
âœ“ Master Agent Coordinator with intelligent task routing
âœ“ Real-time performance optimization and load balancing
âœ“ Cross-service communication and data sharing
âœ“ Enterprise monitoring and analytics dashboard
âœ“ Self-healing capabilities and emergency protocols
âœ“ Multi-domain knowledge integration
âœ“ Advanced decision making with reinforcement learning
âœ“ Enterprise security and compliance enforcement

@Author: Sheily MCP Enterprise System
@Version: 2025.1.0
"""

import asyncio
import json
import logging
import uuid
from collections import defaultdict, deque
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional

# Core system imports - using correct paths
try:
    from ...agents.base.enhanced_base import (
        AgentStatus,
        AgentTask,
        EnhancedBaseMCPAgent,
        TaskPriority,
    )
    from ...agents.coordination.ml_coordinator_advanced import (
        AdvancedBanditArm,
        DeepContextualBandit,
    )
    from ...agents.specialized.finance_agent import FinanceAgent
    from ...utils.multi_modal_processor import MultiModalProcessor

    AGENTS_AVAILABLE = True
except ImportError as e:
    AGENTS_AVAILABLE = False
    # Fallback definitions
    AdvancedBanditArm = None
    DeepContextualBandit = None
    FinanceAgent = None
    MultiModalProcessor = None
    EnhancedBaseMCPAgent = object

try:
    from .self_healing_system import SelfHealingSystem
except ImportError:
    SelfHealingSystem = None

# Basic agent types (fallback if needed)
from enum import Enum

if not AGENTS_AVAILABLE:

    class TaskPriority(Enum):
        LOW = 1
        MEDIUM = 3
        HIGH = 5
        CRITICAL = 7

    class AgentStatus(Enum):
        IDLE = "idle"
        BUSY = "busy"
        ERROR = "error"
        OFFLINE = "offline"


logger = logging.getLogger(__name__)


@dataclass
class SystemMetrics:
    """Real-time system performance tracking"""

    total_requests: int = 0
    processed_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    system_uptime: float = 0.0
    active_agents: int = 0
    active_tasks: int = 0
    total_agents: int = 0

    def success_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.processed_requests / self.total_requests

    def failure_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.failed_requests / self.total_requests


@dataclass
class OrchestratorConfig:
    """Master orchestrator configuration"""

    max_concurrent_tasks: int = 100
    task_timeout_seconds: int = 300
    load_balancing_algorithm: str = "advanced_bandit"
    auto_scaling_enabled: bool = True
    self_healing_enabled: bool = True
    monitoring_interval: int = 30  # seconds
    emergency_protocol_threshold: float = 0.8  # 80% failure rate triggers emergency


class MasterMCPOrchestrator:
    """
    MASTER MCP ORCHESTRATOR - The Heart of Sheily MCP Enterprise
    MODIFICADO: Integra funcionalidad de mÃºltiples orchestradores + 4 agentes consolidados

    Coordinates all system components with intelligent decision making:
    - Advanced ML-based task routing
    - Real-time performance optimization
    - Cross-service communication
    - Enterprise monitoring and analytics
    - Self-healing capabilities
    - 4 MEGA-CONSOLIDATED AGENTS routing
    """

    def __init__(self, config: OrchestratorConfig = None):
        self.config = config or OrchestratorConfig()

        # ===== CONSOLIDATION: Initialize 4 Mega Agents =====
        self._initialize_consolidated_agents()

        # Legacy components set to None for compatibility
        self.multi_modal = None
        self.corpus_integration = None

        # Agent registry and task management
        self.agent_registry: Dict[str, Any] = (
            {}
        )  # Compatible with different agent types
        self.active_tasks: Dict[str, Dict] = {}  # Simplified task structure
        self.task_queue: asyncio.Queue = asyncio.Queue()
        self.completed_tasks: deque = deque(maxlen=10000)

        # System state and monitoring
        self.system_metrics = SystemMetrics()
        self.system_status = "initializing"
        self.start_time = datetime.now()
        self.last_monitoring = datetime.now()

        # Threading and async management
        self.task_processor: Optional[asyncio.Task] = None
        self.monitoring_task: Optional[asyncio.Task] = None
        self.emergency_mode = False

        # Event system for component communication
        self.event_bus = asyncio.Queue()
        self.event_handlers: Dict[str, List[Callable]] = defaultdict(list)

        # Initialize system
        self.setup_system()

    def _initialize_consolidated_agents(self):
        """Initialize the 4 consolidated mega agents"""
        from .consolidated_agents import (
            BusinessAgent,
            CoreAgent,
            InfrastructureAgent,
            MetaCognitionAgent,
        )

        logger.info("ðŸ¤– Initializing 4 Consolidated Mega Agents")

        # Core Agent - handles AI intelligence, LLM, RAG, training
        self.core_agent = CoreAgent(
            agent_id="core_agent_mega", name="Mega Core Intelligence Agent"
        )

        # Business Agent - handles marketplace, payments, analytics
        self.business_agent = BusinessAgent(
            agent_id="business_agent_mega", name="Mega Business Operations Agent"
        )

        # Infrastructure Agent - handles system operations, monitoring, deployment
        self.infrastructure_agent = InfrastructureAgent(
            agent_id="infrastructure_agent_mega",
            name="Mega Infrastructure Operations Agent",
        )

        # Meta-Cognition Agent - handles learning, optimization, coordination
        self.meta_cognition_agent = MetaCognitionAgent(
            agent_id="meta_cognition_agent_mega", name="Mega Meta-Cognition Agent"
        )

        # Register consolidated agents
        self.consolidated_agents = {
            "core": self.core_agent,
            "business": self.business_agent,
            "infrastructure": self.infrastructure_agent,
            "meta_cognition": self.meta_cognition_agent,
        }

        logger.info("âœ… 4 Consolidated Mega Agents initialized successfully")

    def setup_system(self):
        """Initialize all system components"""
        logger.info("ðŸš€ Initializing Sheily MCP Enterprise Master Orchestrator")

        # Register core agents
        self._register_core_agents()

        # Setup event handlers
        self._setup_event_handlers()

        # Initialize monitoring
        self.system_status = "ready"
        logger.info("âœ… Master Orchestrator initialization complete")

    def _register_core_agents(self):
        """Register all core specialized agents"""
        logger.info("ðŸ¤– Registering core agent components")

        # Register consolidated agents instead of legacy agents
        for agent in self.consolidated_agents.values():
            if hasattr(agent, "agent_id"):
                self.agent_registry[agent.agent_id] = agent

        logger.info(f"âœ… Registered {len(self.agent_registry)} core agents")

    def _setup_event_handlers(self):
        """Setup system-wide event handlers"""
        # Task completion events
        self.register_event_handler("task_completed", self._handle_task_completion)
        self.register_event_handler("task_failed", self._handle_task_failure)
        self.register_event_handler(
            "agent_health_changed", self._handle_agent_health_change
        )
        self.register_event_handler("system_anomaly", self._handle_system_anomaly)

    # ================================
    # PUBLIC API METHODS
    # ================================

    async def start_system(self):
        """Start the complete enterprise system"""
        logger.info("ðŸ”¥ Starting Sheily MCP Enterprise System")

        # Start background tasks
        self.task_processor = asyncio.create_task(self._process_tasks())
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())

        # Warm up system
        await self._warm_up_system()

        logger.info("ðŸŽ‰ Sheily MCP Enterprise System started successfully")

    async def stop_system(self):
        """Gracefully shutdown the system"""
        logger.info("ðŸ›‘ Shutting down Sheily MCP Enterprise System")

        # Cancel background tasks
        if self.task_processor:
            self.task_processor.cancel()
        if self.monitoring_task:
            self.monitoring_task.cancel()

        # Save system state
        await self._save_system_state()

        logger.info("âœ… System shutdown complete")

    async def process_task(self, task_request: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main entry point for task processing

        Args:
            task_request: Task specification with requirements

        Returns:
            Task result with execution details
        """
        task_id = str(uuid.uuid4())

        # Create agent task
        task = AgentTask(
            task_id=task_id,
            task_type=task_request.get("task_type", "general"),
            priority=TaskPriority(task_request.get("priority", 3)),
            required_capabilities=task_request.get("capabilities", []),
            timeout_seconds=task_request.get("timeout", 300),
            data=task_request.get("data", {}),
        )

        # Add to processing queue
        await self.task_queue.put(task)
        self.active_tasks[task_id] = task
        self.system_metrics.total_requests += 1

        logger.info(f"ðŸ“‹ Accepted task {task_id}: {task.task_type}")

        # Wait for completion or timeout
        start_time = datetime.now()
        timeout = self.config.task_timeout_seconds

        try:
            # Wait for task completion
            result = await asyncio.wait_for(
                self._wait_for_task(task_id), timeout=timeout
            )

            processing_time = (datetime.now() - start_time).total_seconds()
            self.system_metrics.processed_requests += 1
            self.system_metrics.avg_response_time = (
                self.system_metrics.avg_response_time + processing_time
            ) / 2

            return {
                "task_id": task_id,
                "status": "completed",
                "result": result,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat(),
            }

        except asyncio.TimeoutError:
            self.system_metrics.failed_requests += 1
            logger.warning(f"â° Task {task_id} timed out")

            return {
                "task_id": task_id,
                "status": "timeout",
                "error": "Task processing timeout",
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            self.system_metrics.failed_requests += 1
            logger.error(f"âŒ Task {task_id} failed: {e}")

            return {
                "task_id": task_id,
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        return {
            "system_status": self.system_status,
            "emergency_mode": self.emergency_mode,
            "uptime": (datetime.now() - self.start_time).total_seconds(),
            "metrics": {
                "total_requests": self.system_metrics.total_requests,
                "success_rate": round(self.system_metrics.success_rate(), 3),
                "avg_response_time": round(self.system_metrics.avg_response_time, 2),
                "active_agents": len(
                    [
                        a
                        for a in self.agent_registry.values()
                        if a.status == AgentStatus.RUNNING
                    ]
                ),
                "active_tasks": len(self.active_tasks),
                "total_agents": len(self.agent_registry),
            },
            "agent_health": self._get_agent_health_overview(),
            "consolidated_agents": {
                "core": self.core_agent.status,
                "business": self.business_agent.status,
                "infrastructure": self.infrastructure_agent.status,
                "meta_cognition": self.meta_cognition_agent.status,
            },
        }

    # ================================
    # CORE PROCESSING METHODS
    # ================================

    async def _process_tasks(self):
        """Main task processing loop"""
        logger.info("ðŸ”„ Starting task processing loop")

        while True:
            try:
                # Get next task from queue
                task = await self.task_queue.get()

                if task is None:  # Shutdown signal
                    break

                # Process task asynchronously
                asyncio.create_task(self._execute_task(task))

            except Exception as e:
                logger.error(f"âŒ Task processing error: {e}")
                continue

    async def _execute_task(self, task: AgentTask):
        """Execute a single task with intelligent agent selection"""
        try:
            logger.info(f"ðŸŽ¯ Processing task {task.task_id}: {task.task_type}")

            # Step 1: Intelligent agent selection
            selected_agent = await self._select_optimal_agent(task)

            if not selected_agent:
                await self._emit_event(
                    "task_failed",
                    {"task_id": task.task_id, "reason": "no_suitable_agent"},
                )
                return

            # Step 2: Execute task
            start_time = datetime.now()

            try:
                # CONSOLIDATED AGENT PROCESSING
                # Convert AgentTask to simple dict for consolidated agents
                task_request = {
                    "type": task.task_type,
                    "data": task.data if hasattr(task, "data") else {},
                    "capabilities": (
                        task.required_capabilities
                        if hasattr(task, "required_capabilities")
                        else []
                    ),
                    "task_id": task.task_id,
                }

                result = await selected_agent.process_request(task_request)

                processing_time = (datetime.now() - start_time).total_seconds()

                # Step 3: Update learning systems
                await self._update_learning_systems_consolidated(
                    task, selected_agent, result, processing_time, success=True
                )

                # Step 4: Emit completion event
                await self._emit_event(
                    "task_completed",
                    {
                        "task_id": task.task_id,
                        "agent_id": selected_agent.agent_id,
                        "result": result,
                        "processing_time": processing_time,
                    },
                )

            except Exception as e:
                processing_time = (datetime.now() - start_time).total_seconds()

                # Update with failure
                await self._update_learning_systems_consolidated(
                    task, selected_agent, None, processing_time, success=False
                )

                await self._emit_event(
                    "task_failed",
                    {
                        "task_id": task.task_id,
                        "agent_id": selected_agent.agent_id,
                        "error": str(e),
                        "processing_time": processing_time,
                    },
                )

        except Exception as e:
            logger.error(f"âŒ Task execution error for {task.task_id}: {e}")

        finally:
            # Cleanup
            if task.task_id in self.active_tasks:
                del self.active_tasks[task.task_id]

    async def _select_optimal_agent(self, task: AgentTask) -> Optional[Any]:
        """
        CONSOLIDATED ROUTING - Select best agent from 4 mega agents
        Routes tasks to one of: Core, Business, Infrastructure, MetaCognition
        """

        # Extract task characteristics for routing decision
        task_type = (
            getattr(task, "task_type", "general")
            if hasattr(task, "task_type")
            else "general"
        )
        required_capabilities = (
            getattr(task, "required_capabilities", [])
            if hasattr(task, "required_capabilities")
            else []
        )

        # INTELLIGENT ROUTING to 4 mega agents
        routing_decision = self._determine_mega_agent_routing(
            task_type, required_capabilities
        )

        # Route to appropriate consolidated agent
        if routing_decision == "core":
            logger.info(f"ðŸ§  Routing to CORE AGENT: {task_type}")
            return self.core_agent
        elif routing_decision == "business":
            logger.info(f"ðŸ’¼ Routing to BUSINESS AGENT: {task_type}")
            return self.business_agent
        elif routing_decision == "infrastructure":
            logger.info(f"ðŸ”§ Routing to INFRASTRUCTURE AGENT: {task_type}")
            return self.infrastructure_agent
        elif routing_decision == "meta_cognition":
            logger.info(f"ðŸŽ¯ Routing to META-COGNITION AGENT: {task_type}")
            return self.meta_cognition_agent
        else:
            # Default to core agent if no clear routing decision
            logger.info(f"ðŸŽ² Default routing to CORE AGENT: {task_type}")
            return self.core_agent

    def _determine_mega_agent_routing(
        self, task_type: str, capabilities: List[str]
    ) -> str:
        """Intelligent routing decision to 4 mega agents"""

        # CORE AGENT - AI Intelligence tasks
        core_keywords = [
            "chat",
            "llm",
            "rag",
            "search",
            "analyze",
            "training",
            "model",
            "constitutional",
            "memory",
            "learning",
            "ai",
            "intelligence",
            "generation",
            "completion",
            "embedding",
            "neural",
        ]

        # BUSINESS AGENT - Business operations
        business_keywords = [
            "marketplace",
            "payment",
            "order",
            "user",
            "analytics",
            "revenue",
            "token",
            "sheilys",
            "customer",
            "transaction",
            "business",
            "sale",
            "billing",
            "subscription",
            "commerce",
        ]

        # INFRASTRUCTURE AGENT - System operations
        infrastructure_keywords = [
            "monitor",
            "performance",
            "resource",
            "backup",
            "security",
            "database",
            "api",
            "container",
            "docker",
            "system",
            "deployment",
            "infrastructure",
            "server",
            "network",
            "storage",
            "scaling",
        ]

        # META-COGNITION AGENT - Learning and coordination
        meta_keywords = [
            "coordinate",
            "optimize",
            "route",
            "balance",
            "learn",
            "improve",
            "meta",
            "cognition",
            "orchestrate",
            "plan",
            "strategy",
            "evolution",
            "adaptation",
            "quality",
            "efficiency",
        ]

        # Count matches for each agent type
        core_score = sum(1 for keyword in core_keywords if keyword in task_type.lower())
        business_score = sum(
            1 for keyword in business_keywords if keyword in task_type.lower()
        )
        infrastructure_score = sum(
            1 for keyword in infrastructure_keywords if keyword in task_type.lower()
        )
        meta_score = sum(1 for keyword in meta_keywords if keyword in task_type.lower())

        # Also check capabilities
        for cap in capabilities:
            cap_lower = cap.lower()
            if any(keyword in cap_lower for keyword in core_keywords):
                core_score += 1
            elif any(keyword in cap_lower for keyword in business_keywords):
                business_score += 1
            elif any(keyword in cap_lower for keyword in infrastructure_keywords):
                infrastructure_score += 1
            elif any(keyword in cap_lower for keyword in meta_keywords):
                meta_score += 1

        # Return agent with highest score
        scores = {
            "core": core_score,
            "business": business_score,
            "infrastructure": infrastructure_score,
            "meta_cognition": meta_score,
        }

        # Get the agent type with maximum score
        best_agent = max(scores.items(), key=lambda x: x[1])

        logger.info(
            f"ðŸ“Š Routing scores - Core:{core_score}, Business:{business_score}, Infrastructure:{infrastructure_score}, Meta:{meta_score}"
        )

        return best_agent[0]

        logger.info(
            f"ðŸŽ¯ Selected agent {best_agent.agent_id} for task {task.task_type} (score: {best_score:.3f})"
        )
        return best_agent

    def _agent_meets_requirements(self, agent, task) -> bool:
        """Check if agent meets task requirements for consolidated agents"""
        # For consolidated agents, always return True as they handle all capabilities
        return True

    async def _update_learning_systems_consolidated(
        self, task, agent, result: Any, processing_time: float, success: bool
    ):
        """Update learning systems for consolidated agents"""

        # Simple metrics tracking for consolidated agents
        agent.load_metrics["requests"] += 1
        if success:
            agent.load_metrics["success"] += 1
        else:
            agent.load_metrics["errors"] += 1

        # Update system metrics
        self.system_metrics.total_requests += 1
        if success:
            self.system_metrics.successful_requests += 1
        else:
            self.system_metrics.failed_requests += 1

        logger.info(
            f"ðŸ“Š Agent {agent.agent_id} - Success: {agent.load_metrics['success']}, Errors: {agent.load_metrics['errors']}"
        )

    async def _update_learning_systems(
        self,
        task: AgentTask,
        agent: EnhancedBaseMCPAgent,
        result: Any,
        processing_time: float,
        success: bool,
    ):
        """Update all learning systems with execution feedback"""

        # Reward calculation
        reward = 1.0 if success else 0.0

        # Latency penalty/bonus
        if processing_time < 10:
            reward += 0.1  # Bonus for fast execution
        elif processing_time > 60:
            reward -= 0.2  # Penalty for slow execution

        # Quality adjustment (if available)
        if result and isinstance(result, dict) and "quality_score" in result:
            reward += (result["quality_score"] - 0.5) * 0.2

        # Context vector extraction
        context = self.contextual_bandit.extract_context_vector(task, agent)

        # Update contextual bandit
        self.contextual_bandit.update(agent.agent_id, context, reward, task.task_type)

        # Update ML coordinator
        self.ml_coordinator.update(reward, processing_time)

    # ================================
    # MONITORING AND SELF-HEALING
    # ================================

    async def _monitoring_loop(self):
        """Continuous system monitoring and health checks"""
        logger.info("ðŸ“Š Starting system monitoring loop")

        while True:
            try:
                await asyncio.sleep(self.config.monitoring_interval)

                # System health check
                await self._perform_system_health_check()

                # Self-healing actions if needed
                await self._perform_self_healing_actions()

                # Emergency protocol check
                await self._check_emergency_protocols()

            except Exception as e:
                logger.error(f"âŒ Monitoring error: {e}")
                continue

    async def _perform_system_health_check(self):
        """Comprehensive system health assessment"""
        health_issues = []

        # Check agent health
        unhealthy_agents = []
        for agent_id, agent in self.agent_registry.items():
            if agent.status != AgentStatus.RUNNING:
                unhealthy_agents.append(agent_id)

        if unhealthy_agents:
            health_issues.append(f"Unhealthy agents: {unhealthy_agents}")

        # Check task queue health
        queue_size = self.task_queue.qsize()
        if queue_size > self.config.max_concurrent_tasks * 2:
            health_issues.append(f"Task queue overloaded: {queue_size} tasks")

        # Check system metrics
        failure_rate = self.system_metrics.failure_rate()
        if failure_rate > 0.1:  # 10% failure rate
            health_issues.append(f"High failure rate: {failure_rate:.1%}")

        # Emit health status
        await self._emit_event(
            "system_health_check",
            {
                "issues": health_issues,
                "metrics": {
                    "active_agents": len(
                        [
                            a
                            for a in self.agent_registry.values()
                            if a.status == AgentStatus.RUNNING
                        ]
                    ),
                    "queue_size": queue_size,
                    "failure_rate": failure_rate,
                },
            },
        )

    async def _perform_self_healing_actions(self):
        """Execute self-healing actions for system issues"""
        if not self.config.self_healing_enabled:
            return

        # Use self-healing system for intelligent recovery
        healing_actions = await self.self_healing.analyze_and_heal_system(
            {
                "agent_health": self._get_agent_health_overview(),
                "system_metrics": self.system_metrics.__dict__,
                "recent_failures": (
                    list(self.completed_tasks)[-10:] if self.completed_tasks else []
                ),
            }
        )

        for action in healing_actions:
            try:
                await self._execute_healing_action(action)
                logger.info(f"ðŸ”§ Executed healing action: {action}")
            except Exception as e:
                logger.error(f"âŒ Healing action failed: {action} - {e}")

    async def _execute_healing_action(self, action: Dict[str, Any]):
        """Execute a specific healing action"""
        action_type = action.get("type")

        if action_type == "restart_agent":
            agent_id = action.get("agent_id")
            if agent_id in self.agent_registry:
                await self.agent_registry[agent_id].restart()

        elif action_type == "scale_agents":
            # Scaling logic would involve external orchestration
            agent_type = action.get("agent_type")
            count = action.get("count", 1)
            logger.info(f"âš–ï¸ Scaling request: {agent_type} +{count}")
            # In a real K8s env, this would call the K8s API
            # For now, we simulate by logging the event
            await self._emit_event("scaling_triggered", {"agent_type": agent_type, "count": count})

        elif action_type == "failover_task":
            task_id = action.get("task_id")
            # Implement task failover logic
            if task_id in self.active_tasks:
                task = self.active_tasks[task_id]
                logger.warning(f"ðŸ”„ Initiating failover for task {task_id}")
                # Re-queue task with high priority
                task.priority = TaskPriority.CRITICAL
                await self.task_queue.put(task)
                await self._emit_event("task_failover", {"task_id": task_id})

    async def _check_emergency_protocols(self):
        """Check and activate emergency protocols if needed"""
        failure_rate = self.system_metrics.failure_rate()
        threshold = self.config.emergency_protocol_threshold

        if failure_rate > threshold and not self.emergency_mode:
            logger.warning(
                f"ðŸš¨ EMERGENCY MODE ACTIVATED - Failure rate: {failure_rate:.1%}"
            )
            self.emergency_mode = True

            await self._emit_event(
                "emergency_mode_activated",
                {
                    "trigger": "high_failure_rate",
                    "failure_rate": failure_rate,
                    "threshold": threshold,
                },
            )

        elif failure_rate < threshold * 0.7 and self.emergency_mode:
            logger.info(
                f"âœ… EMERGENCY MODE DEACTIVATED - Failure rate: {failure_rate:.1%}"
            )
            self.emergency_mode = False

            await self._emit_event(
                "emergency_mode_deactivated",
                {"trigger": "failure_rate_normalized", "failure_rate": failure_rate},
            )

    # ================================
    # EVENT SYSTEM
    # ================================

    def register_event_handler(self, event_type: str, handler: Callable):
        """Register an event handler"""
        self.event_handlers[event_type].append(handler)

    async def _emit_event(self, event_type: str, data: Dict[str, Any]):
        """Emit a system-wide event"""
        event = {
            "type": event_type,
            "data": data,
            "timestamp": datetime.now().isoformat(),
        }

        # Add to event bus
        await self.event_bus.put(event)

        # Call registered handlers
        for handler in self.event_handlers[event_type]:
            try:
                await handler(event)
            except Exception as e:
                logger.error(f"âŒ Event handler error for {event_type}: {e}")

    # ================================
    # UTILITY METHODS
    # ================================

    async def _wait_for_task(self, task_id: str) -> Dict[str, Any]:
        """Wait for task completion using event-driven coordination"""
        # Create a future to wait for task completion
        completion_future = asyncio.Future()

        # Set up event handler for this specific task
        def task_completion_handler(event):
            if event["data"].get("task_id") == task_id:
                if event["type"] == "task_completed":
                    completion_future.set_result({
                        "status": "completed",
                        "result": event["data"].get("result", {}),
                        "processing_time": event["data"].get("processing_time", 0),
                        "agent_id": event["data"].get("agent_id")
                    })
                elif event["type"] == "task_failed":
                    completion_future.set_exception(Exception(
                        event["data"].get("error", "Task execution failed")
                    ))

        # Register temporary handler for this task
        self.register_event_handler("task_completed", task_completion_handler)
        self.register_event_handler("task_failed", task_completion_handler)

        try:
            # Wait for task completion
            result = await completion_future
            return result

        except Exception as e:
            logger.error(f"Error waiting for task {task_id}: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "task_id": task_id
            }

        finally:
            # Clean up event handlers
            try:
                self.event_handlers["task_completed"].remove(task_completion_handler)
                self.event_handlers["task_failed"].remove(task_completion_handler)
            except ValueError:
                pass  # Handler might have been removed already

    def _get_agent_health_overview(self) -> Dict[str, Any]:
        """Get overview of agent health status"""
        total_agents = len(self.agent_registry)
        running_agents = len(
            [a for a in self.agent_registry.values() if a.status == AgentStatus.RUNNING]
        )
        idle_agents = len(
            [a for a in self.agent_registry.values() if a.status == AgentStatus.IDLE]
        )

        return {
            "total_agents": total_agents,
            "running_agents": running_agents,
            "idle_agents": idle_agents,
            "failed_agents": total_agents - running_agents - idle_agents,
            "health_score": running_agents / total_agents if total_agents > 0 else 0,
        }

    async def _warm_up_system(self):
        """Warm up the system with initial tasks"""
        logger.info("ðŸ”¥ Warming up system components")

        # Quick health check of all agents
        for agent_id, agent in self.agent_registry.items():
            try:
                await agent.health_check()
            except Exception as e:
                logger.warning(f"âš ï¸ Agent {agent_id} health check failed: {e}")

        logger.info("âœ… System warm-up complete")

    async def _save_system_state(self):
        """Save current system state for persistence"""
        state = {
            "system_metrics": self.system_metrics.__dict__,
            "agent_registry_size": len(self.agent_registry),
            "active_tasks_count": len(self.active_tasks),
            "timestamp": datetime.now().isoformat(),
        }

        # Save to file (in production, this would be to a database)
        state_file = "config/system_state.json"
        with open(state_file, "w") as f:
            json.dump(state, f, indent=2, default=str)

        logger.info(f"ðŸ’¾ System state saved to {state_file}")

    # ================================
    # EVENT HANDLERS
    # ================================

    async def _handle_task_completion(self, event: Dict[str, Any]):
        """Handle task completion event"""
        task_id = event["data"]["task_id"]
        agent_id = event["data"]["agent_id"]
        processing_time = event["data"]["processing_time"]

        # Store completed task
        self.completed_tasks.append(
            {
                "task_id": task_id,
                "agent_id": agent_id,
                "status": "completed",
                "processing_time": processing_time,
                "timestamp": datetime.now(),
            }
        )

        logger.info(
            f"âœ… Task {task_id} completed by agent {agent_id} in {processing_time:.2f}s"
        )

    async def _handle_task_failure(self, event: Dict[str, Any]):
        """Handle task failure event"""
        task_id = event["data"]["task_id"]
        error = event["data"]["error"]

        # Store failed task
        self.completed_tasks.append(
            {
                "task_id": task_id,
                "status": "failed",
                "error": error,
                "timestamp": datetime.now(),
            }
        )

        logger.warning(f"âŒ Task {task_id} failed: {error}")

    async def _handle_agent_health_change(self, event: Dict[str, Any]):
        """Handle agent health status change"""
        agent_id = event["data"]["agent_id"]
        status = event["data"]["status"]

        logger.info(f"ðŸ¥ Agent {agent_id} health changed to: {status}")

        # Trigger healing actions if needed
        if status in ["failed", "unhealthy", "degraded"]:
            await self.self_healing.trigger_healing_action(
                "agent_health_failure", {"agent_id": agent_id, "status": status}
            )

    async def _handle_system_anomaly(self, event: Dict[str, Any]):
        """Handle system anomaly detection"""
        anomaly_type = event["data"]["anomaly_type"]
        severity = event["data"]["severity"]

        logger.warning(
            f"ðŸš¨ System anomaly detected: {anomaly_type} (severity: {severity})"
        )

        if severity > 0.8:  # Critical anomaly
            await self._activate_emergency_mode(anomaly_type)

    async def _activate_emergency_mode(self, trigger: str):
        """Activate emergency mode for critical system issues"""
        logger.warning(f"ðŸš¨ EMERGENCY MODE ACTIVATED - Trigger: {trigger}")

        self.emergency_mode = True

        # Emergency actions
        # 1. Limit concurrent tasks
        # 2. Redirect to failover agents
        # 3. Notify administrators
        # 4. Trigger system recovery

        await self._emit_event(
            "emergency_mode_activated",
            {"trigger": trigger, "timestamp": datetime.now().isoformat()},
        )


# ================================
# GLOBAL ORCHESTRATOR INSTANCE
# ================================

_global_orchestrator = None


def get_master_orchestrator() -> MasterMCPOrchestrator:
    """Get the global master orchestrator instance"""
    global _global_orchestrator
    if _global_orchestrator is None:
        config = OrchestratorConfig(
            max_concurrent_tasks=50,
            task_timeout_seconds=600,
            auto_scaling_enabled=True,
            self_healing_enabled=True,
            monitoring_interval=15,
        )
        _global_orchestrator = MasterMCPOrchestrator(config)
    return _global_orchestrator


async def initialize_system():
    """Initialize the complete Sheily MCP Enterprise system"""
    orchestrator = get_master_orchestrator()
    await orchestrator.start_system()
    return orchestrator


if __name__ == "__main__":
    # For testing purposes
    print("ðŸŽ¯ Sheily MCP Enterprise - Master Orchestrator")
    print(
        "Run: python -c \"from sheily_core.master_orchestrator import get_master_orchestrator; print('âœ… Orchestrator loaded')\""
    )
