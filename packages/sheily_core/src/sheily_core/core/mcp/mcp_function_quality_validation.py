#!/usr/bin/env python3
"""
VALIDACI√ìN DE CALIDAD DE ASIGNACIONES - An√°lisis detallado de 100% coverage
===========================================================================

Sistema que valida y analiza la calidad de las asignaciones autom√°ticas realizadas
por el MCP Function Assignment Orchestrator.
"""

import asyncio
import json
from datetime import datetime


async def validate_mcp_assignments():
    """Analizar y validar todas las asignaciones de funciones a agentes"""

    # Importar el orchestrator (asume que est√° cargado)
    from mcp_function_assignments import get_function_assignment_orchestrator

    orchestrator = get_function_assignment_orchestrator()

    if not orchestrator.agent_assignments:
        # Simular asignaciones completas si no est√°n cargadas
        print("üì• Cargando asignaciones del sistema MCP...")
        await orchestrator.initialize_complete_assignment_system()

    # An√°lisis completo
    total_functions = len(orchestrator.function_registry)
    assigned_functions = len(orchestrator.agent_assignments)
    coverage = (assigned_functions / total_functions * 100) if total_functions > 0 else 0

    print("üéØ MCP FUNCTION ASSIGNMENT QUALITY ANALYSIS")
    print("=" * 80)
    print(f"üìä TOTAL FUNCTIONS: {total_functions}")
    print(f"ü§ñ ASSIGNED FUNCTIONS: {assigned_functions}")
    print(f"üìà COVERAGE: {coverage:.1f}%")
    # An√°lisis detallado de calidad
    assignment_analysis = await analyze_assignment_quality(orchestrator.agent_assignments)
    agent_distribution = await get_agent_distribution(orchestrator.agent_assignments)

    print(f"\nüéØ QUALITY METRICS:")
    print(f"   üìä Average capabilities per function: {assignment_analysis['avg_capabilities']:.2f}")
    print(f"   üéØ High-confidence assignments: {assignment_analysis['high_confidence']}")
    print(f"   üîÑ Unique agent types used: {assignment_analysis['unique_agents']}")

    print(f"\nü§ñ AGENT SPECIALIZATION TOP 10:")
    for i, (agent, count) in enumerate(agent_distribution[:10], 1):
        pct = (count / assigned_functions) * 100 if assigned_functions > 0 else 0
        print(f"   {i:2d}. {agent}: {count} ({pct:.1f}%)")
    quality_validation = await validate_logic_assignment_quality(orchestrator.agent_assignments)

    # Calcular puntuaci√≥n general de calidad
    overall_quality_score = calculate_overall_quality_score(
        quality_validation, assignment_analysis, coverage
    )

    print(f"\n‚úÖ QUALITY VALIDATION RESULTS:")
    print(f"   ‚ÑπÔ∏è Validation checks performed: {quality_validation['checks_performed']}")
    print(f"   ‚ö†Ô∏è Quality issues found: {quality_validation['issues_found']}")

    if quality_validation['issues_found'] > 0:
        print("   üìã ISSUES IDENTIFIED:")
        for issue in quality_validation['issues_list'][:5]:  # Show first 5
            print(f"      - {issue}")
        if len(quality_validation['issues_list']) > 5:
            print(f"      ... and {len(quality_validation['issues_list']) - 5} more")
    else:
        print("   üéâ All assignments meet quality standards!")

    # Muestra detallada de asignaciones
    assignments_sample = list(orchestrator.agent_assignments.items())[:20]

    print(f"\nüìã DETAILED ASSIGNMENT SAMPLES (First 20):")
    print("-" * 80)
    for i, (func_path, assignment) in enumerate(assignments_sample, 1):
        agent = assignment['primary_agent']
        capabilities = assignment.get('capabilities', [])
        cap_list = ', '.join(capabilities[:2]) + ('...' if len(capabilities) > 2 else '')
        print(f"   {i:2d}. {func_path} -> {agent} [{cap_list}]")

    print(f"   üåü OVERALL QUALITY SCORE: {overall_quality_score:.0f}/100")
    return {
        'total_functions': total_functions,
        'assigned_functions': assigned_functions,
        'coverage_percentage': coverage,
        'quality_score': overall_quality_score,
        'issues': quality_validation['issues_found'],
        'unique_agents': assignment_analysis['unique_agents'],
        'validation_passed': quality_validation['issues_found'] == 0
    }

async def analyze_assignment_quality(assignments):
    """Analyze the quality of function assignments"""
    if not assignments:
        return {'avg_capabilities': 0, 'high_confidence': 0, 'unique_agents': 0}

    total_assignments = len(assignments)
    agent_set = set()
    high_confidence = 0
    total_capabilities = 0

    for assignment in assignments.values():
        agent_set.add(assignment.get('primary_agent', 'unknown'))
        capabilities = assignment.get('capabilities', [])
        total_capabilities += len(capabilities)
        if len(capabilities) >= 2:
            high_confidence += 1

    return {
        'avg_capabilities': total_capabilities / total_assignments if total_assignments > 0 else 0,
        'high_confidence': high_confidence,
        'unique_agents': len(agent_set)
    }

async def get_agent_distribution(assignments):
    """Get distribution of agents used"""
    if not assignments:
        return []

    agent_count = {}
    for assignment in assignments.values():
        agent = assignment.get('primary_agent', 'unknown')
        agent_count[agent] = agent_count.get(agent, 0) + 1

    return sorted(agent_count.items(), key=lambda x: x[1], reverse=True)

async def validate_logic_assignment_quality(assignments):
    """Validate logical consistency of assignments"""
    issues = []
    checks_performed = 0

    for func_path, assignment in assignments.items():
        agent = assignment.get('primary_agent', '')
        capabilities = assignment.get('capabilities', [])
        checks_performed += 1

        # Check 1: Security functions should get security agents
        if any(word in func_path.lower() for word in ['password', 'auth', 'token', 'encrypt', 'security']):
            if agent not in ['SecurityAgent', 'EncryptionAgent', 'AuthAgent'] and 'security' not in capabilities:
                issues.append(f"Security function {func_path} not assigned security agent")

        # Check 2: Test functions should get testing agents
        if 'test' in func_path.lower():
            if agent not in ['TestAgent', 'QAAgent'] and 'testing' not in capabilities:
                issues.append(f"Test function {func_path} not assigned testing agent")

        # Check 3: ML functions should get ML agents
        if any(word in func_path.lower() for word in ['ml', 'ai', 'predict', 'train', 'model']):
            if agent not in ['MLAIAgent', 'TrainingAgent', 'ModelAgent', 'AIAgent'] and 'machine_learning' not in capabilities:
                issues.append(f"ML function {func_path} not assigned ML agent")

        # Check 4: API functions should get API agents
        if 'api' in func_path.lower() or any(word in func_path.lower() for word in ['endpoint', 'request', 'response']):
            if agent not in ['APIAgent', 'EndpointAgent', 'RESTAgent'] and 'api' not in capabilities:
                issues.append(f"API function {func_path} not assigned API agent")

        # Check 5: Blockchain functions should get blockchain agents
        if any(word in func_path.lower() for word in ['blockchain', 'crypto', 'token', 'web3']):
            if agent not in ['BlockchainAgent', 'CryptoAgent', 'TokenAgent'] and 'blockchain' not in capabilities:
                issues.append(f"Blockchain function {func_path} not assigned blockchain agent")

    return {
        'checks_performed': checks_performed,
        'issues_found': len(issues),
        'issues_list': issues,
        'quality_score': max(0, 100 - (len(issues) / checks_performed * 100) if checks_performed > 0 else 0)  # Lower issues = higher score
    }

def calculate_overall_quality_score(quality_validation, assignment_analysis, coverage):
    """Calculate overall quality score (0-100)"""
    # Coverage score (30% weight)
    coverage_score = min(100, coverage)

    # Assignment quality score (40% weight)
    quality_score = min(100, max(0, 100 - quality_validation.get('issues_found', 0)))

    # Agent diversity score (20% weight)
    diversity_score = min(100, assignment_analysis.get('unique_agents', 0) * 10)

    # Average capabilities score (10% weight)
    capability_score = min(100, assignment_analysis.get('avg_capabilities', 0) * 20)

    # Weighted average
    overall_score = (
        coverage_score * 0.3 +
        quality_score * 0.4 +
        diversity_score * 0.2 +
        capability_score * 0.1
    )

    return overall_score

# Execute validation if run directly
if __name__ == "__main__":
    async def main():
        print("üîç MCP FUNCTION ASSIGNMENT QUALITY VALIDATION")
        print("Validating 100% coverage assignment quality...")

        try:
            results = await validate_mcp_assignments()

            print(f"\nüéØ VALIDATION COMPLETE:")
            print(f"   ‚úÖ Quality Score: {results['quality_score']:.0f}/100")
            print(f"   ‚úÖ Functions Cover: {results['coverage_percentage']:.1f}%")
            print(f"   ‚úÖ Issues Found: {results['issues']}")
            print(f"   ‚úÖ Agents Used: {results['unique_agents']}")
            print(f"   ‚úÖ Validation Result: {'PASSED' if results['validation_passed'] else 'NEEDS IMPROVEMENT'}")

            if results['quality_score'] >= 95:
                print("\nüåü RESULT: MILITARY-GRADE ASSIGNMENT QUALITY ACHIEVED!")
                print("   The Auto-Assignment AI has proven its effectiveness")
                print("   100% coverage with enterprise-grade accuracy")
            else:
                print(f"\n‚ö†Ô∏è RESULT: Quality score {results['quality_score']:.0f}/100")
                print("   Some optimizations may improve assignment logic")

        except Exception as e:
            print(f"‚ùå Validation error: {e}")
            print("   Please ensure mcp_function_assignments.py has been run first")

    asyncio.run(main())
