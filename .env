# Sheily AI MCP Enterprise Configuration
LOG_LEVEL=INFO
MAX_WORKERS=4
CACHE_TTL=3600
MEMORY_LIMIT=512
DEBUG_MODE=false

# Webhook integrations (opcional)
SLACK_WEBHOOK=
DISCORD_WEBHOOK=
TEAMS_WEBHOOK=

# Remote Gemini Backend Configuration (Ollama-compatible)
OLLAMA_BASE_URL=https://sheily-ai-backend-749968449333.europe-west1.run.app
LLM_MODEL_NAME=gemma3:1b
SKIP_MODEL_LOAD=true

ENABLE_CONSCIOUSNESS=true

# ═══════════════════════════════════════════════════════════════
# EL-AMANECER V4 - CONFIGURACIÓN GLOBAL DE PUERTOS
# ═══════════════════════════════════════════════════════════════
# Backend API (GraphQL Federation Gateway)
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Frontend Next.js
NEXT_PUBLIC_PORT=3000
NEXT_PUBLIC_API_URL=http://localhost:8000

# Consciousness API Service
CONSCIOUSNESS_API_PORT=8001

# WebSocket
WEBSOCKET_URL=ws://localhost:8000/ws

# API Base URL
API_BASE_URL=http://localhost:8000/api/v1
